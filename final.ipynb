{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b7077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74199efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 20:31:39 | INFO | No command flag supplied → defaulting to --train\n",
      "2025-06-29 20:31:39 | INFO | ============================================================\n",
      "2025-06-29 20:31:39 | INFO | ENHANCED UAV WIND PREDICTION PIPELINE\n",
      "2025-06-29 20:31:39 | INFO | ============================================================\n",
      "2025-06-29 20:31:39 | INFO | Training models for pressure levels: (1000, 950, 900, 850) hPa\n",
      "2025-06-29 20:31:39 | INFO | Data source: C:\\Users\\Victus\\Downloads\\953c78511a142e4b81be9c53038b62a5.grib\n",
      "2025-06-29 20:31:39 | INFO | Loading GRIB data...\n",
      "2025-06-29 20:31:39 | WARNING | Ignoring index file 'C:\\\\Users\\\\Victus\\\\Downloads\\\\953c78511a142e4b81be9c53038b62a5.grib.5b7b6.idx' incompatible with GRIB file\n",
      "2025-06-29 20:32:55 | INFO | Raw data dimensions: {'time': 1464, 'isobaricInhPa': 4, 'latitude': 15, 'longitude': 17}\n",
      "2025-06-29 20:32:55 | INFO | Starting comprehensive feature engineering...\n",
      "2025-06-29 20:33:04 | INFO | Engineering features for 4 pressure levels...\n",
      "2025-06-29 20:33:23 | INFO | Computing inter-level features for 6 level pairs\n",
      "2025-06-29 20:34:43 | INFO | Feature engineering complete. Total variables: 115\n",
      "2025-06-29 20:34:43 | INFO | Converting to DataFrame with optimization...\n",
      "2025-06-29 20:34:43 | INFO | Applying thinning: spatial stride=2, temporal stride=6\n",
      "2025-06-29 20:34:43 | INFO | Grid after thinning: {'time': 244, 'isobaricInhPa': 4, 'latitude': 8, 'longitude': 9}\n",
      "2025-06-29 20:34:53 | INFO | DataFrame shape after optimization: (70272, 122)\n",
      "2025-06-29 20:34:53 | INFO | ==================================================\n",
      "2025-06-29 20:34:53 | INFO | TRAINING MODEL FOR 1000 hPa\n",
      "2025-06-29 20:34:53 | INFO | ==================================================\n",
      "2025-06-29 20:34:53 | INFO | Training model for 1000 hPa...\n",
      "2025-06-29 20:34:53 | INFO | Temporal split: 56,217 train, 14,055 test samples\n",
      "2025-06-29 20:34:53 | ERROR | Failed to train model for 1000 hPa: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.TimeDelta64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)\n",
      "2025-06-29 20:34:53 | INFO | ==================================================\n",
      "2025-06-29 20:34:53 | INFO | TRAINING MODEL FOR 950 hPa\n",
      "2025-06-29 20:34:53 | INFO | ==================================================\n",
      "2025-06-29 20:34:53 | INFO | Training model for 950 hPa...\n",
      "2025-06-29 20:34:53 | INFO | Temporal split: 56,217 train, 14,055 test samples\n",
      "2025-06-29 20:34:53 | ERROR | Failed to train model for 950 hPa: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.TimeDelta64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)\n",
      "2025-06-29 20:34:53 | INFO | ==================================================\n",
      "2025-06-29 20:34:53 | INFO | TRAINING MODEL FOR 900 hPa\n",
      "2025-06-29 20:34:53 | INFO | ==================================================\n",
      "2025-06-29 20:34:53 | INFO | Training model for 900 hPa...\n",
      "2025-06-29 20:34:54 | INFO | Temporal split: 56,217 train, 14,055 test samples\n",
      "2025-06-29 20:34:54 | ERROR | Failed to train model for 900 hPa: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.TimeDelta64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)\n",
      "2025-06-29 20:34:54 | INFO | ==================================================\n",
      "2025-06-29 20:34:54 | INFO | TRAINING MODEL FOR 850 hPa\n",
      "2025-06-29 20:34:54 | INFO | ==================================================\n",
      "2025-06-29 20:34:54 | INFO | Training model for 850 hPa...\n",
      "2025-06-29 20:34:54 | INFO | Temporal split: 56,217 train, 14,055 test samples\n",
      "2025-06-29 20:34:54 | ERROR | Failed to train model for 850 hPa: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.TimeDelta64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)\n",
      "2025-06-29 20:34:54 | INFO | Training summary saved: models\\training_summary.json\n",
      "2025-06-29 20:34:54 | INFO | ============================================================\n",
      "2025-06-29 20:34:54 | INFO | TRAINING COMPLETED!\n",
      "2025-06-29 20:34:54 | INFO | ============================================================\n",
      "2025-06-29 20:34:54 | INFO | Successfully trained 0 models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 PERFORMANCE SUMMARY:\n",
      "Level (hPa)  Test R²    Test RMSE    Overfitting \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import itertools\n",
    "import warnings\n",
    "import argparse\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for the UAV wind prediction pipeline.\"\"\"\n",
    "    grib_path: Path = Path(r\"C:\\Users\\Victus\\Downloads\\953c78511a142e4b81be9c53038b62a5.grib\")\n",
    "    pressure_levels: Tuple[int, ...] = (1000, 950, 900, 850)  # hPa, sorted high to low\n",
    "    g: float = 9.80665  # Gravity constant (m/s^2)\n",
    "    \n",
    "    # Data processing optimization\n",
    "    spatial_stride: int = 2        # Keep every 2nd lat/lon point\n",
    "    temporal_stride: int = 6       # Keep every 6th hour\n",
    "    \n",
    "    # Model parameters\n",
    "    test_ratio: float = 0.20\n",
    "    random_state: int = 42\n",
    "    n_estimators: int = 200\n",
    "    max_depth: int = 15\n",
    "    min_samples_split: int = 10\n",
    "    min_samples_leaf: int = 5\n",
    "    \n",
    "    # Output paths\n",
    "    models_dir: Path = Path(\"./models\")\n",
    "    logs_dir: Path = Path(\"./logs\")\n",
    "    \n",
    "    # Feature engineering options\n",
    "    enable_lstm: bool = False\n",
    "    sequence_length: int = 24\n",
    "    hyperparameter_tuning: bool = False\n",
    "\n",
    "# Global configuration instance\n",
    "CFG = Config()\n",
    "\n",
    "# ========================================\n",
    "# ENHANCED FEATURE ENGINEERING\n",
    "# ========================================\n",
    "def engineer_features_comprehensive(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering with all atmospheric relationships.\n",
    "    Processes all levels at once for efficiency.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting comprehensive feature engineering...\")\n",
    "    \n",
    "    # Ensure we have the required pressure levels\n",
    "    available_levels = sorted([int(p) for p in ds.isobaricInhPa.values], reverse=True)\n",
    "    target_levels = [p for p in CFG.pressure_levels if p in available_levels]\n",
    "    \n",
    "    if len(target_levels) != len(CFG.pressure_levels):\n",
    "        logger.warning(f\"Only {len(target_levels)} of {len(CFG.pressure_levels)} levels available\")\n",
    "    \n",
    "    # Select required pressure levels\n",
    "    ds = ds.sel(isobaricInhPa=target_levels)\n",
    "    \n",
    "    # Calculate base wind speed for all levels\n",
    "    ds[\"wind_speed\"] = np.hypot(ds.u, ds.v)\n",
    "    \n",
    "    logger.info(f\"Engineering features for {len(target_levels)} pressure levels...\")\n",
    "    \n",
    "    # ===== LEVEL-SPECIFIC FEATURES =====\n",
    "    for p in target_levels:\n",
    "        # Primary wind variables\n",
    "        ds[f\"ws{p}\"] = ds.wind_speed.sel(isobaricInhPa=p)\n",
    "        ds[f\"u{p}\"] = ds.u.sel(isobaricInhPa=p)\n",
    "        ds[f\"v{p}\"] = ds.v.sel(isobaricInhPa=p)\n",
    "        \n",
    "        # Wind direction (meteorological convention)\n",
    "        ang = np.arctan2(ds.v.sel(isobaricInhPa=p), ds.u.sel(isobaricInhPa=p))\n",
    "        ds[f\"wd{p}\"] = (270 - np.degrees(ang)) % 360\n",
    "        \n",
    "        # Temperature (K to °C)\n",
    "        ds[f\"temp{p}\"] = ds.t.sel(isobaricInhPa=p) - 273.15\n",
    "        \n",
    "        # Geopotential height (m)\n",
    "        ds[f\"height{p}\"] = ds.z.sel(isobaricInhPa=p) / CFG.g\n",
    "        \n",
    "        # Optional atmospheric variables (if available)\n",
    "        optional_vars = {\n",
    "            'w': f'w{p}',           # Vertical velocity\n",
    "            'pv': f'pv{p}',         # Potential vorticity\n",
    "            'vo': f'vort{p}',       # Relative vorticity\n",
    "            'd': f'div{p}',         # Divergence\n",
    "            'q': f'q{p}',           # Specific humidity\n",
    "            'r': f'r{p}'            # Relative humidity\n",
    "        }\n",
    "        \n",
    "        for var, name in optional_vars.items():\n",
    "            if var in ds.data_vars:\n",
    "                ds[name] = ds[var].sel(isobaricInhPa=p)\n",
    "    \n",
    "    # ===== INTER-LEVEL FEATURES =====\n",
    "    # Compute relationships between all level pairs\n",
    "    level_pairs = [(b, t) for b, t in itertools.combinations(target_levels, 2) if b > t]\n",
    "    logger.info(f\"Computing inter-level features for {len(level_pairs)} level pairs\")\n",
    "    \n",
    "    for bottom, top in level_pairs:\n",
    "        # Wind shear magnitude (vector difference)\n",
    "        du = ds[f\"u{top}\"] - ds[f\"u{bottom}\"]\n",
    "        dv = ds[f\"v{top}\"] - ds[f\"v{bottom}\"]\n",
    "        ds[f\"shear_{bottom}_{top}\"] = np.hypot(du, dv)\n",
    "        \n",
    "        # Directional wind shear\n",
    "        ang_bottom = np.arctan2(ds[f\"v{bottom}\"], ds[f\"u{bottom}\"])\n",
    "        ang_top = np.arctan2(ds[f\"v{top}\"], ds[f\"u{top}\"])\n",
    "        ddir = ang_top - ang_bottom\n",
    "        # Handle angle wrapping\n",
    "        ddir = np.where(ddir > np.pi, ddir - 2 * np.pi, ddir)\n",
    "        ddir = np.where(ddir < -np.pi, ddir + 2 * np.pi, ddir)\n",
    "        ds[f\"dirshear_{bottom}_{top}\"] = ((\"time\", \"latitude\", \"longitude\"), ddir)\n",
    "        \n",
    "        # Temperature lapse rate (°C/km)\n",
    "        dz = ds[f\"height{top}\"] - ds[f\"height{bottom}\"]\n",
    "        dt = ds[f\"temp{bottom}\"] - ds[f\"temp{top}\"]\n",
    "        ds[f\"lapse_{bottom}_{top}\"] = (dt / dz) * 1000  # Convert to °C/km\n",
    "        \n",
    "        # Height and temperature differences\n",
    "        ds[f\"dz_{bottom}_{top}\"] = dz\n",
    "        ds[f\"dt_{bottom}_{top}\"] = dt\n",
    "        \n",
    "        # Wind speed differences\n",
    "        ds[f\"dws_{bottom}_{top}\"] = ds[f\"ws{top}\"] - ds[f\"ws{bottom}\"]\n",
    "    \n",
    "    # ===== ATMOSPHERIC COLUMN FEATURES =====\n",
    "    # Mean properties across all levels\n",
    "    ds[\"temp_column_mean\"] = ds.t.mean(\"isobaricInhPa\") - 273.15\n",
    "    ds[\"ws_column_mean\"] = ds.wind_speed.mean(\"isobaricInhPa\")\n",
    "    ds[\"u_column_mean\"] = ds.u.mean(\"isobaricInhPa\")\n",
    "    ds[\"v_column_mean\"] = ds.v.mean(\"isobaricInhPa\")\n",
    "    \n",
    "    # Atmospheric stability indicators\n",
    "    if len(target_levels) >= 2:\n",
    "        # Bulk Richardson number approximation\n",
    "        surface_level = max(target_levels)\n",
    "        upper_level = min(target_levels)\n",
    "        \n",
    "        dtheta = (ds[f\"temp{upper_level}\"] - ds[f\"temp{surface_level}\"]) + \\\n",
    "                 (CFG.g / 1004) * (ds[f\"height{upper_level}\"] - ds[f\"height{surface_level}\"])\n",
    "        du_bulk = ds[f\"u{upper_level}\"] - ds[f\"u{surface_level}\"]\n",
    "        dv_bulk = ds[f\"v{upper_level}\"] - ds[f\"v{surface_level}\"]\n",
    "        wind_shear_bulk = np.hypot(du_bulk, dv_bulk)\n",
    "        \n",
    "        ds[\"bulk_richardson\"] = (CFG.g * dtheta * (ds[f\"height{upper_level}\"] - ds[f\"height{surface_level}\"])) / \\\n",
    "                               (ds[f\"temp{surface_level}\"] * wind_shear_bulk**2 + 1e-10)\n",
    "    \n",
    "    # Column-integrated features (if humidity available)\n",
    "    if 'q' in ds.data_vars:\n",
    "        ds[\"q_column_mean\"] = ds.q.mean(\"isobaricInhPa\")\n",
    "        # Precipitable water approximation\n",
    "        dp_levels = np.diff(sorted(target_levels, reverse=True))\n",
    "        if len(dp_levels) > 0:\n",
    "            ds[\"precipitable_water\"] = (ds.q * np.mean(dp_levels) * 100 / CFG.g).sum(\"isobaricInhPa\")\n",
    "    \n",
    "    if 'r' in ds.data_vars:\n",
    "        ds[\"rh_column_mean\"] = ds.r.mean(\"isobaricInhPa\")\n",
    "    \n",
    "    # ===== GEOGRAPHIC AND TEMPORAL FEATURES =====\n",
    "    # Geographic coordinates\n",
    "    ds[\"lat\"] = ds.latitude\n",
    "    ds[\"lon\"] = ds.longitude\n",
    "    \n",
    "    # Terrain approximation (using surface geopotential if available)\n",
    "    if len(target_levels) > 0:\n",
    "        surface_level = max(target_levels)\n",
    "        ds[\"terrain_height\"] = ds[f\"height{surface_level}\"]\n",
    "    \n",
    "    # Time-based features with cyclic encoding\n",
    "    if 'time' in ds.dims:\n",
    "        time_vals = pd.to_datetime(ds.time.values)\n",
    "        \n",
    "        # Basic time features\n",
    "        ds[\"hour\"] = ((\"time\",), time_vals.hour.values)\n",
    "        ds[\"day_of_year\"] = ((\"time\",), time_vals.dayofyear.values)\n",
    "        ds[\"month\"] = ((\"time\",), time_vals.month.values)\n",
    "        \n",
    "        # Cyclic encodings\n",
    "        ds[\"hour_sin\"] = np.sin(2 * np.pi * ds[\"hour\"] / 24)\n",
    "        ds[\"hour_cos\"] = np.cos(2 * np.pi * ds[\"hour\"] / 24)\n",
    "        ds[\"doy_sin\"] = np.sin(2 * np.pi * ds[\"day_of_year\"] / 365)\n",
    "        ds[\"doy_cos\"] = np.cos(2 * np.pi * ds[\"day_of_year\"] / 365)\n",
    "        ds[\"month_sin\"] = np.sin(2 * np.pi * ds[\"month\"] / 12)\n",
    "        ds[\"month_cos\"] = np.cos(2 * np.pi * ds[\"month\"] / 12)\n",
    "    \n",
    "    logger.info(f\"Feature engineering complete. Total variables: {len(ds.data_vars)}\")\n",
    "    return ds\n",
    "\n",
    "def get_features_excluding_target(ds: xr.Dataset, target_level: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get feature names while excluding target level to prevent data leakage.\n",
    "    This is the key leakage prevention mechanism from Script 1.\n",
    "    \"\"\"\n",
    "    # Variables to always exclude\n",
    "    exclude_base = {'wind_speed', 'u', 'v', 't', 'z', 'time', 'step', 'surface', \n",
    "                   'latitude', 'longitude', 'isobaricInhPa'}\n",
    "    \n",
    "    # Target variable name\n",
    "    target_var = f\"ws{target_level}\"\n",
    "    \n",
    "    # All variables that contain the target level (to prevent leakage)\n",
    "    level_specific_exclusions = {var for var in ds.data_vars.keys() \n",
    "                                if str(target_level) in var and var != target_var}\n",
    "    \n",
    "    # Combine all exclusions\n",
    "    all_exclusions = exclude_base | level_specific_exclusions | {target_var}\n",
    "    \n",
    "    # Get valid features\n",
    "    valid_features = [var for var in ds.data_vars.keys() if var not in all_exclusions]\n",
    "    \n",
    "    logger.info(f\"Target level {target_level} hPa: {len(valid_features)} features \"\n",
    "               f\"(excluded {len(level_specific_exclusions)} level-specific vars)\")\n",
    "    \n",
    "    return valid_features\n",
    "\n",
    "# ========================================\n",
    "# EFFICIENT DATA PROCESSING\n",
    "# ========================================\n",
    "def apply_spatial_temporal_thinning(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Apply spatial and temporal thinning for computational efficiency.\"\"\"\n",
    "    logger.info(f\"Applying thinning: spatial stride={CFG.spatial_stride}, \"\n",
    "               f\"temporal stride={CFG.temporal_stride}\")\n",
    "    \n",
    "    # Spatial thinning\n",
    "    if CFG.spatial_stride > 1:\n",
    "        lat_indices = slice(None, None, CFG.spatial_stride)\n",
    "        lon_indices = slice(None, None, CFG.spatial_stride)\n",
    "        ds = ds.isel(latitude=lat_indices, longitude=lon_indices)\n",
    "    \n",
    "    # Temporal thinning\n",
    "    if CFG.temporal_stride > 1 and 'time' in ds.dims:\n",
    "        time_indices = slice(None, None, CFG.temporal_stride)\n",
    "        ds = ds.isel(time=time_indices)\n",
    "    \n",
    "    logger.info(f\"Grid after thinning: {dict(ds.dims)}\")\n",
    "    return ds\n",
    "\n",
    "def to_dataframe_optimized(ds: xr.Dataset) -> pd.DataFrame:\n",
    "    \"\"\"Convert xarray to pandas with memory optimization.\"\"\"\n",
    "    logger.info(\"Converting to DataFrame with optimization...\")\n",
    "    \n",
    "    # Apply thinning first\n",
    "    ds_thin = apply_spatial_temporal_thinning(ds)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = ds_thin.to_dataframe().dropna().reset_index()\n",
    "    \n",
    "    logger.info(f\"DataFrame shape after optimization: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# ========================================\n",
    "# ENHANCED MODEL TRAINING\n",
    "# ========================================\n",
    "def temporal_train_test_split(df: pd.DataFrame, test_ratio: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Temporal split to prevent data leakage in time series.\n",
    "    \"\"\"\n",
    "    if 'time' in df.columns:\n",
    "        df_sorted = df.sort_values('time')\n",
    "        split_idx = int(len(df_sorted) * (1 - test_ratio))\n",
    "        train_df = df_sorted.iloc[:split_idx]\n",
    "        test_df = df_sorted.iloc[split_idx:]\n",
    "        logger.info(f\"Temporal split: {len(train_df):,} train, {len(test_df):,} test samples\")\n",
    "        return train_df, test_df\n",
    "    else:\n",
    "        logger.warning(\"No time column found, using random split\")\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        return train_test_split(df, test_size=test_ratio, random_state=CFG.random_state)\n",
    "\n",
    "def hyperparameter_optimization(X_train: np.ndarray, y_train: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform hyperparameter optimization using RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting hyperparameter optimization...\")\n",
    "    \n",
    "    param_distributions = {\n",
    "        'n_estimators': [100, 150, 200, 250],\n",
    "        'max_depth': [10, 15, 20, 25, None],\n",
    "        'min_samples_split': [5, 10, 15, 20],\n",
    "        'min_samples_leaf': [2, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    \n",
    "    rf_base = RandomForestRegressor(random_state=CFG.random_state, n_jobs=-1, oob_score=True)\n",
    "    \n",
    "    # Use TimeSeriesSplit for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        rf_base, \n",
    "        param_distributions,\n",
    "        n_iter=50,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=CFG.random_state,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    logger.info(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    logger.info(f\"Best CV score: {-search.best_score_:.4f}\")\n",
    "    \n",
    "    return search.best_params_\n",
    "\n",
    "def train_enhanced_model(df: pd.DataFrame, target_level: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train an enhanced model for a specific pressure level.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Training model for {target_level} hPa...\")\n",
    "    \n",
    "    # Get features (excluding target level to prevent leakage)\n",
    "    # We need to reconstruct the dataset to get proper feature exclusion\n",
    "    target_var = f\"ws{target_level}\"\n",
    "    \n",
    "    # Get all features except those containing the target level\n",
    "    exclude_patterns = {target_var, 'wind_speed', 'u', 'v', 't', 'z'}\n",
    "    exclude_with_level = [col for col in df.columns if str(target_level) in col and col != target_var]\n",
    "    all_exclusions = exclude_patterns | set(exclude_with_level)\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col not in all_exclusions]\n",
    "    \n",
    "    # Prepare data\n",
    "    df_model = df[feature_cols + [target_var]].copy()\n",
    "    \n",
    "    # Temporal split\n",
    "    train_df, test_df = temporal_train_test_split(df_model, CFG.test_ratio)\n",
    "    \n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[target_var]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[target_var]\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Hyperparameter optimization (if enabled)\n",
    "    if CFG.hyperparameter_tuning:\n",
    "        best_params = hyperparameter_optimization(X_train_scaled, y_train)\n",
    "    else:\n",
    "        best_params = {\n",
    "            'n_estimators': CFG.n_estimators,\n",
    "            'max_depth': CFG.max_depth,\n",
    "            'min_samples_split': CFG.min_samples_split,\n",
    "            'min_samples_leaf': CFG.min_samples_leaf\n",
    "        }\n",
    "    \n",
    "    # Train final model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        **best_params,\n",
    "        oob_score=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=CFG.random_state\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred_train = rf_model.predict(X_train_scaled)\n",
    "    y_pred_test = rf_model.predict(X_test_scaled)\n",
    "    \n",
    "    performance = {\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
    "        'oob_r2': rf_model.oob_score_\n",
    "    }\n",
    "    \n",
    "    # Check for overfitting\n",
    "    overfitting_gap = performance['train_r2'] - performance['test_r2']\n",
    "    performance['overfitting_detected'] = overfitting_gap > 0.1\n",
    "    \n",
    "    logger.info(f\"Level {target_level} hPa - Test R²: {performance['test_r2']:.3f}, \"\n",
    "               f\"Test RMSE: {performance['test_rmse']:.3f} m/s\")\n",
    "    \n",
    "    if performance['overfitting_detected']:\n",
    "        logger.warning(f\"Possible overfitting detected (R² gap: {overfitting_gap:.3f})\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save model artifacts\n",
    "    CFG.models_dir.mkdir(exist_ok=True)\n",
    "    model_path = CFG.models_dir / f\"rf_{target_level}hPa.pkl\"\n",
    "    scaler_path = CFG.models_dir / f\"scaler_{target_level}hPa.pkl\"\n",
    "    \n",
    "    joblib.dump(rf_model, model_path, compress=3)\n",
    "    joblib.dump(scaler, scaler_path, compress=3)\n",
    "    \n",
    "    logger.info(f\"Saved model: {model_path}\")\n",
    "    logger.info(f\"Saved scaler: {scaler_path}\")\n",
    "    \n",
    "    return {\n",
    "        'model': rf_model,\n",
    "        'scaler': scaler,\n",
    "        'features': feature_cols,\n",
    "        'performance': performance,\n",
    "        'feature_importance': feature_importance,\n",
    "        'hyperparameters': best_params\n",
    "    }\n",
    "\n",
    "# ========================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# ========================================\n",
    "def train_all_models() -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Train models for all pressure levels with enhanced features.\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"ENHANCED UAV WIND PREDICTION PIPELINE\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Training models for pressure levels: {CFG.pressure_levels} hPa\")\n",
    "    logger.info(f\"Data source: {CFG.grib_path}\")\n",
    "    \n",
    "    # Load and process data\n",
    "    logger.info(\"Loading GRIB data...\")\n",
    "    ds_raw = xr.open_dataset(CFG.grib_path, engine=\"cfgrib\")\n",
    "    logger.info(f\"Raw data dimensions: {dict(ds_raw.dims)}\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    ds_features = engineer_features_comprehensive(ds_raw)\n",
    "    \n",
    "    # Convert to DataFrame with optimization\n",
    "    df = to_dataframe_optimized(ds_features)\n",
    "    \n",
    "    # Train models for each level\n",
    "    trained_models = {}\n",
    "    \n",
    "    for target_level in CFG.pressure_levels:\n",
    "        logger.info(\"=\"*50)\n",
    "        logger.info(f\"TRAINING MODEL FOR {target_level} hPa\")\n",
    "        logger.info(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            model_info = train_enhanced_model(df, target_level)\n",
    "            trained_models[target_level] = model_info\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to train model for {target_level} hPa: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {}\n",
    "    for level, info in trained_models.items():\n",
    "        summary[level] = info['performance']\n",
    "        summary[level]['hyperparameters'] = info['hyperparameters']\n",
    "    \n",
    "    summary_path = CFG.models_dir / \"training_summary.json\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(f\"Training summary saved: {summary_path}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"TRAINING COMPLETED!\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Successfully trained {len(trained_models)} models\")\n",
    "    \n",
    "    print(\"\\n📊 PERFORMANCE SUMMARY:\")\n",
    "    print(f\"{'Level (hPa)':<12} {'Test R²':<10} {'Test RMSE':<12} {'Overfitting':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for level, info in trained_models.items():\n",
    "        perf = info['performance']\n",
    "        status = \"⚠️ High\" if perf['overfitting_detected'] else \"✅ Low\"\n",
    "        print(f\"{level:<12} {perf['test_r2']:<10.3f} {perf['test_rmse']:<12.3f} {status}\")\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "# ========================================\n",
    "# ALTITUDE TO PRESSURE LEVEL MAPPING\n",
    "# ========================================\n",
    "def altitude_to_pressure_level(altitude_m: float) -> int:\n",
    "    \"\"\"\n",
    "    Convert altitude in meters to appropriate pressure level.\n",
    "    Uses standard atmosphere approximation.\n",
    "    \"\"\"\n",
    "    if altitude_m < 100:\n",
    "        return 1000\n",
    "    elif altitude_m < 600:\n",
    "        return 950\n",
    "    elif altitude_m < 1200:\n",
    "        return 900\n",
    "    else:\n",
    "        return 850\n",
    "\n",
    "# ========================================\n",
    "# PREDICTION FUNCTIONS\n",
    "# ========================================\n",
    "def load_trained_models() -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"Load all trained models from disk.\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    for level in CFG.pressure_levels:\n",
    "        model_path = CFG.models_dir / f\"rf_{level}hPa.pkl\"\n",
    "        scaler_path = CFG.models_dir / f\"scaler_{level}hPa.pkl\"\n",
    "        \n",
    "        if model_path.exists() and scaler_path.exists():\n",
    "            models[level] = {\n",
    "                'model': joblib.load(model_path),\n",
    "                'scaler': joblib.load(scaler_path)\n",
    "            }\n",
    "            logger.info(f\"Loaded model for {level} hPa\")\n",
    "        else:\n",
    "            logger.warning(f\"Model files not found for {level} hPa\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_wind_along_path(path_points: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Predict wind speeds along a UAV flight path.\n",
    "    \n",
    "    Args:\n",
    "        path_points: List of dictionaries with keys:\n",
    "            - 'time': timestamp\n",
    "            - 'latitude': latitude in degrees\n",
    "            - 'longitude': longitude in degrees  \n",
    "            - 'altitude_m': altitude in meters\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions with wind speed estimates\n",
    "    \"\"\"\n",
    "    logger.info(f\"Predicting wind for {len(path_points)} path points...\")\n",
    "    \n",
    "    # Load models\n",
    "    models = load_trained_models()\n",
    "    \n",
    "    if not models:\n",
    "        logger.error(\"No trained models available\")\n",
    "        return []\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for point in path_points:\n",
    "        # Determine appropriate pressure level\n",
    "        target_level = altitude_to_pressure_level(point['altitude_m'])\n",
    "        \n",
    "        if target_level not in models:\n",
    "            logger.warning(f\"No model available for pressure level {target_level} hPa\")\n",
    "            predictions.append({\n",
    "                **point,\n",
    "                'pressure_level': target_level,\n",
    "                'wind_speed_ms': None,\n",
    "                'prediction_status': 'no_model'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # In a real implementation, you would:\n",
    "        # 1. Load weather data for the point's time/location\n",
    "        # 2. Engineer features using the same process\n",
    "        # 3. Apply the appropriate model\n",
    "        \n",
    "        # For now, return structure with placeholder\n",
    "        predictions.append({\n",
    "            **point,\n",
    "            'pressure_level': target_level,\n",
    "            'wind_speed_ms': None,  # Would be actual prediction\n",
    "            'prediction_status': 'model_available'\n",
    "        })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ========================================\n",
    "# COMMAND LINE INTERFACE\n",
    "# ========================================\n",
    "def create_cli() -> argparse.ArgumentParser:\n",
    "    \"\"\"Create command line interface.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Enhanced UAV Wind Prediction Pipeline\",\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    \n",
    "    # Main commands\n",
    "    parser.add_argument(\"--train\", action=\"store_true\", \n",
    "                       help=\"Train models for all pressure levels\")\n",
    "    parser.add_argument(\"--predict-demo\", action=\"store_true\",\n",
    "                       help=\"Run a demonstration prediction\")\n",
    "    \n",
    "    # Configuration options\n",
    "    parser.add_argument(\"--grib-path\", type=Path, default=CFG.grib_path,\n",
    "                       help=\"Path to GRIB data file\")\n",
    "    parser.add_argument(\"--models-dir\", type=Path, default=CFG.models_dir,\n",
    "                       help=\"Directory to save/load models\")\n",
    "    parser.add_argument(\"--spatial-stride\", type=int, default=CFG.spatial_stride,\n",
    "                       help=\"Spatial thinning stride\")\n",
    "    parser.add_argument(\"--temporal-stride\", type=int, default=CFG.temporal_stride,\n",
    "                       help=\"Temporal thinning stride\")\n",
    "    parser.add_argument(\"--hyperparameter-tuning\", action=\"store_true\",\n",
    "                       help=\"Enable hyperparameter optimization\")\n",
    "    \n",
    "    return parser\n",
    "\n",
    "# ========================================\n",
    "# MAIN ENTRY POINT\n",
    "# ========================================\n",
    "def main() -> None:\n",
    "    \"\"\"Parse CLI, update configuration, and run selected command.\"\"\"\n",
    "    parser = create_cli()\n",
    "\n",
    "    # tolerant parse → unknown flags (e.g. from Jupyter) won't abort\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        logger.debug(\"Ignored unknown CLI flags: %s\", unknown)\n",
    "\n",
    "    # ── update runtime configuration ──────────────────────────────\n",
    "    CFG.grib_path              = args.grib_path\n",
    "    CFG.models_dir             = args.models_dir\n",
    "    CFG.spatial_stride         = args.spatial_stride\n",
    "    CFG.temporal_stride        = args.temporal_stride\n",
    "    CFG.hyperparameter_tuning  = args.hyperparameter_tuning\n",
    "\n",
    "    CFG.models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    CFG.logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ── command routing ───────────────────────────────────────────\n",
    "    if args.train:\n",
    "        logger.info(\"▶ TRAIN selected ─ starting model training…\")\n",
    "        train_all_models()\n",
    "        logger.info(\"✓ Training finished\")\n",
    "\n",
    "    elif args.predict_demo:\n",
    "        logger.info(\"▶ PREDICT-DEMO selected ─ running demo…\")\n",
    "        demo_path = [\n",
    "            dict(time=pd.Timestamp('2023-01-01 12:00'),\n",
    "                 latitude=25.0, longitude=77.0, altitude_m=120),\n",
    "            dict(time=pd.Timestamp('2023-01-01 13:00'),\n",
    "                 latitude=25.1, longitude=77.1, altitude_m=200)\n",
    "        ]\n",
    "        preds = predict_wind_along_path(demo_path)\n",
    "\n",
    "        print(\"\\n🎯 DEMO PREDICTIONS\")\n",
    "        for p in preds:\n",
    "            print(f\" lat={p['latitude']:.2f}, lon={p['longitude']:.2f}, \"\n",
    "                  f\"alt={p['altitude_m']} m  →  \"\n",
    "                  f\"{p['pressure_level']} hPa | {p['prediction_status']} | \"\n",
    "                  f\"wind={p['wind_speed_ms']}\")\n",
    "        print()\n",
    "\n",
    "    else:\n",
    "        # parser.print_help()\n",
    "        logger.info(\"No command flag supplied → defaulting to --train\")\n",
    "        train_all_models()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# standard entry point\n",
    "# ------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c07d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "UAV WIND PREDICTION PIPELINE - MULTI-LEVEL APPROACH\n",
      "============================================================\n",
      "Training models for pressure levels: [1000, 950, 900, 850] hPa\n",
      "Data source: C:/Users/Victus/Downloads/953c78511a142e4b81be9c53038b62a5.grib\n",
      "Starting UAV Wind Prediction Pipeline...\n",
      "\n",
      "========================================\n",
      "STEP 1: LOADING AND PROCESSING DATA\n",
      "========================================\n",
      "→ Loading GRIB file...\n",
      "  Raw data shape: {'time': 1464, 'isobaricInhPa': 4, 'latitude': 15, 'longitude': 17}\n",
      "\n",
      "==================================================\n",
      "TRAINING MODEL FOR 1000 hPa\n",
      "==================================================\n",
      "→ Engineering features...\n",
      "  → Engineering features for 1000 hPa target (excluding 1000 hPa data)\n",
      "  → Using feature levels: [950, 900, 850]\n",
      "  → Computing inter-level features for 3 level pairs\n",
      "  → Created 69 features\n",
      "→ Creating DataFrame...\n",
      "  → DataFrame shape: (1493280, 77)\n",
      "→ Checking for data leakage...\n",
      "  ✅ No obvious data leakage detected\n",
      "→ Splitting data temporally...\n",
      "  → Temporal split: 1194624 train, 298656 test samples\n",
      "→ Scaling features...\n",
      "→ Training Random Forest...\n",
      "→ Evaluating model...\n",
      "\n",
      "  📊 MODEL PERFORMANCE FOR 1000 hPa:\n",
      "     Training   - RMSE: 0.210, R²: 0.964, MAE: 0.145\n",
      "     Testing    - RMSE: 0.312, R²: 0.799, MAE: 0.219\n",
      "     OOB Score  - R²: 0.961\n",
      "     ⚠️  Possible overfitting detected (train-test R² gap: 0.165)\n",
      "→ Analyzing feature importance...\n",
      "  Top 5 most important features:\n",
      "     ws950               : 0.5381\n",
      "     wsmean_excl_target  : 0.1205\n",
      "     hour_sin            : 0.0793\n",
      "     ws900               : 0.0736\n",
      "     lon                 : 0.0183\n",
      "  💾 Saved model: wind_model_1000hPa.pkl\n",
      "  💾 Saved scaler: scaler_1000hPa.pkl\n",
      "\n",
      "==================================================\n",
      "TRAINING MODEL FOR 950 hPa\n",
      "==================================================\n",
      "→ Engineering features...\n",
      "  → Engineering features for 950 hPa target (excluding 950 hPa data)\n",
      "  → Using feature levels: [1000, 900, 850]\n",
      "  → Computing inter-level features for 3 level pairs\n",
      "  → Created 90 features\n",
      "→ Creating DataFrame...\n",
      "  → DataFrame shape: (1493280, 98)\n",
      "→ Checking for data leakage...\n",
      "  ✅ No obvious data leakage detected\n",
      "→ Splitting data temporally...\n",
      "  → Temporal split: 1194624 train, 298656 test samples\n",
      "→ Scaling features...\n",
      "→ Training Random Forest...\n",
      "→ Evaluating model...\n",
      "\n",
      "  📊 MODEL PERFORMANCE FOR 950 hPa:\n",
      "     Training   - RMSE: 0.142, R²: 0.996, MAE: 0.074\n",
      "     Testing    - RMSE: 0.152, R²: 0.994, MAE: 0.078\n",
      "     OOB Score  - R²: 0.996\n",
      "     ✅ Good generalization (train-test R² gap: 0.002)\n",
      "→ Analyzing feature importance...\n",
      "  Top 5 most important features:\n",
      "     ws1000              : 0.4378\n",
      "     u950                : 0.3742\n",
      "     v950                : 0.1872\n",
      "     wd950               : 0.0003\n",
      "     pv1000              : 0.0002\n",
      "  💾 Saved model: wind_model_950hPa.pkl\n",
      "  💾 Saved scaler: scaler_950hPa.pkl\n",
      "\n",
      "==================================================\n",
      "TRAINING MODEL FOR 900 hPa\n",
      "==================================================\n",
      "→ Engineering features...\n",
      "  → Engineering features for 900 hPa target (excluding 900 hPa data)\n",
      "  → Using feature levels: [1000, 950, 850]\n",
      "  → Computing inter-level features for 3 level pairs\n",
      "  → Created 95 features\n",
      "→ Creating DataFrame...\n",
      "  → DataFrame shape: (1493280, 103)\n",
      "→ Checking for data leakage...\n",
      "  ✅ No obvious data leakage detected\n",
      "→ Splitting data temporally...\n",
      "  → Temporal split: 1194624 train, 298656 test samples\n",
      "→ Scaling features...\n",
      "→ Training Random Forest...\n",
      "→ Evaluating model...\n",
      "\n",
      "  📊 MODEL PERFORMANCE FOR 900 hPa:\n",
      "     Training   - RMSE: 0.105, R²: 0.999, MAE: 0.065\n",
      "     Testing    - RMSE: 0.175, R²: 0.993, MAE: 0.110\n",
      "     OOB Score  - R²: 0.999\n",
      "     ✅ Good generalization (train-test R² gap: 0.006)\n",
      "→ Analyzing feature importance...\n",
      "  Top 5 most important features:\n",
      "     shear_1000_900      : 0.7039\n",
      "     wsmean_excl_target  : 0.1552\n",
      "     u900                : 0.0424\n",
      "     ws1000              : 0.0346\n",
      "     dirshear_1000_900   : 0.0277\n",
      "  💾 Saved model: wind_model_900hPa.pkl\n",
      "  💾 Saved scaler: scaler_900hPa.pkl\n",
      "\n",
      "==================================================\n",
      "TRAINING MODEL FOR 850 hPa\n",
      "==================================================\n",
      "→ Engineering features...\n",
      "  → Engineering features for 850 hPa target (excluding 850 hPa data)\n",
      "  → Using feature levels: [1000, 950, 900]\n",
      "  → Computing inter-level features for 3 level pairs\n",
      "  → Created 95 features\n",
      "→ Creating DataFrame...\n",
      "  → DataFrame shape: (1493280, 103)\n",
      "→ Checking for data leakage...\n",
      "  ✅ No obvious data leakage detected\n",
      "→ Splitting data temporally...\n",
      "  → Temporal split: 1194624 train, 298656 test samples\n",
      "→ Scaling features...\n",
      "→ Training Random Forest...\n",
      "→ Evaluating model...\n",
      "\n",
      "  📊 MODEL PERFORMANCE FOR 850 hPa:\n",
      "     Training   - RMSE: 0.084, R²: 0.999, MAE: 0.044\n",
      "     Testing    - RMSE: 0.103, R²: 0.998, MAE: 0.056\n",
      "     OOB Score  - R²: 0.999\n",
      "     ✅ Good generalization (train-test R² gap: 0.001)\n",
      "→ Analyzing feature importance...\n",
      "  Top 5 most important features:\n",
      "     ws900               : 0.4397\n",
      "     u850                : 0.2252\n",
      "     shear_1000_850      : 0.1960\n",
      "     v850                : 0.1051\n",
      "     dirshear_1000_850   : 0.0173\n",
      "  💾 Saved model: wind_model_850hPa.pkl\n",
      "  💾 Saved scaler: scaler_850hPa.pkl\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED!\n",
      "============================================================\n",
      "✅ Successfully trained 4 models\n",
      "✅ Models available for pressure levels: [1000, 950, 900, 850] hPa\n",
      "\n",
      "📊 OVERALL PERFORMANCE SUMMARY:\n",
      "Level (hPa)  Test R²    Test RMSE    Overfitting \n",
      "--------------------------------------------------\n",
      "1000         0.799      0.312        ⚠️ High\n",
      "950          0.994      0.152        ✅ Low\n",
      "900          0.993      0.175        ✅ Low\n",
      "850          0.998      0.103        ✅ Low\n",
      "\n",
      "🎯 Next steps:\n",
      "   1. Use predict_wind_along_path() function for UAV route planning\n",
      "   2. Load models with joblib.load() for deployment\n",
      "   3. Integrate with real-time weather data feeds\n",
      "\n",
      "============================================================\n",
      "PIPELINE READY FOR DEPLOYMENT!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========================================\n",
    "# CONSTANTS AND CONFIGURATION\n",
    "# ========================================\n",
    "GRIB_PATH = \"C:/Users/Victus/Downloads/953c78511a142e4b81be9c53038b62a5.grib\"\n",
    "PRESSURE_LEVELS = [1000, 950, 900, 850]  # hPa, sorted high to low\n",
    "G = 9.80665  # Gravity (m/s^2)\n",
    "MODELS_TO_TRAIN = PRESSURE_LEVELS  # We'll train models for all levels\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UAV WIND PREDICTION PIPELINE - MULTI-LEVEL APPROACH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training models for pressure levels: {MODELS_TO_TRAIN} hPa\")\n",
    "print(f\"Data source: {GRIB_PATH}\")\n",
    "\n",
    "# ========================================\n",
    "# STEP 1: FEATURE ENGINEERING (NO LEAKAGE)\n",
    "# ========================================\n",
    "def engineer_features_safe(ds: xr.Dataset, target_level: int) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Engineer meteorological features WITHOUT data leakage for a specific target level.\n",
    "    \n",
    "    Key principle: NEVER use information from the target pressure level when \n",
    "    predicting that level's wind speed.\n",
    "    \"\"\"\n",
    "    print(f\"  → Engineering features for {target_level} hPa target (excluding {target_level} hPa data)\")\n",
    "    \n",
    "    # Calculate wind speed for all levels first\n",
    "    ds[\"wind_speed\"] = np.hypot(ds.u, ds.v)\n",
    "    \n",
    "    # Get available levels (excluding target to prevent leakage)\n",
    "    feature_levels = [p for p in PRESSURE_LEVELS if p != target_level]\n",
    "    print(f\"  → Using feature levels: {feature_levels}\")\n",
    "    \n",
    "    # ===== LEVEL-SPECIFIC FEATURES (excluding target level) =====\n",
    "    for p in feature_levels:\n",
    "        # Wind speed and components\n",
    "        ds[f\"ws{p}\"] = ds.wind_speed.sel(isobaricInhPa=p)\n",
    "        ds[f\"u{p}\"] = ds.u.sel(isobaricInhPa=p)\n",
    "        ds[f\"v{p}\"] = ds.v.sel(isobaricInhPa=p)\n",
    "        \n",
    "        # Wind direction\n",
    "        ang = np.arctan2(ds.v.sel(isobaricInhPa=p), ds.u.sel(isobaricInhPa=p))\n",
    "        ds[f\"wd{p}\"] = (270 - np.degrees(ang)) % 360\n",
    "        \n",
    "        # Temperature (K to °C)\n",
    "        ds[f\"temp{p}\"] = ds.t.sel(isobaricInhPa=p) - 273.15\n",
    "        \n",
    "        # Geopotential height\n",
    "        ds[f\"height{p}\"] = ds.z.sel(isobaricInhPa=p) / G\n",
    "        \n",
    "        # Optional atmospheric variables (if available)\n",
    "        for var, name in [('w', 'w'), ('pv', 'pv'), ('vo', 'vort'), \n",
    "                         ('d', 'div'), ('q', 'q'), ('r', 'r')]:\n",
    "            if var in ds.data_vars:\n",
    "                ds[f\"{name}{p}\"] = ds[var].sel(isobaricInhPa=p)\n",
    "    \n",
    "    # ===== INTER-LEVEL FEATURES (excluding target level) =====\n",
    "    # Only compute between non-target levels\n",
    "    pairs = [(b, t) for b, t in itertools.combinations(feature_levels, 2) if b > t]\n",
    "    print(f\"  → Computing inter-level features for {len(pairs)} level pairs\")\n",
    "    \n",
    "    for b, t in pairs:\n",
    "        # Wind shear magnitude\n",
    "        du = ds[f\"u{t}\"] - ds[f\"u{b}\"]\n",
    "        dv = ds[f\"v{t}\"] - ds[f\"v{b}\"]\n",
    "        ds[f\"shear_{b}_{t}\"] = np.hypot(du, dv)\n",
    "        \n",
    "        # Directional shear\n",
    "        ang_b = np.arctan2(ds[f\"v{b}\"], ds[f\"u{b}\"])\n",
    "        ang_t = np.arctan2(ds[f\"v{t}\"], ds[f\"u{t}\"])\n",
    "        ddir = ang_t - ang_b\n",
    "        ddir = np.where(ddir > np.pi, ddir - 2 * np.pi, ddir)\n",
    "        ddir = np.where(ddir < -np.pi, ddir + 2 * np.pi, ddir)\n",
    "        ds[f\"dirshear_{b}_{t}\"] = ((\"time\", \"latitude\", \"longitude\"), ddir)\n",
    "        \n",
    "        # Temperature lapse rate (°C/km)\n",
    "        lapse = (ds[f\"temp{b}\"] - ds[f\"temp{t}\"]) / (ds[f\"height{t}\"] - ds[f\"height{b}\"]) * 1000\n",
    "        ds[f\"lapse_{b}_{t}\"] = lapse\n",
    "        \n",
    "        # Height and temperature differences\n",
    "        ds[f\"dz_{b}_{t}\"] = ds[f\"height{t}\"] - ds[f\"height{b}\"]\n",
    "        ds[f\"dt_{b}_{t}\"] = ds[f\"temp{b}\"] - ds[f\"temp{t}\"]\n",
    "    \n",
    "    # ===== AGGREGATE FEATURES (excluding target level) =====\n",
    "    if len(feature_levels) > 1:\n",
    "        # Mean atmospheric properties across available levels\n",
    "        ds[\"tmean_excl_target\"] = ds.t.sel(isobaricInhPa=feature_levels).mean(\"isobaricInhPa\") - 273.15\n",
    "        ds[\"wsmean_excl_target\"] = ds.wind_speed.sel(isobaricInhPa=feature_levels).mean(\"isobaricInhPa\")\n",
    "        \n",
    "        # Atmospheric column characteristics\n",
    "        if 'q' in ds.data_vars:\n",
    "            ds[\"qmean_excl_target\"] = ds.q.sel(isobaricInhPa=feature_levels).mean(\"isobaricInhPa\")\n",
    "        if 'r' in ds.data_vars:\n",
    "            ds[\"rmean_excl_target\"] = ds.r.sel(isobaricInhPa=feature_levels).mean(\"isobaricInhPa\")\n",
    "    \n",
    "    # ===== GEOGRAPHIC AND TEMPORAL FEATURES =====\n",
    "    # These don't cause leakage as they're independent of target\n",
    "    ds[\"lat\"] = ds.latitude\n",
    "    ds[\"lon\"] = ds.longitude\n",
    "    \n",
    "    # Time-based features (if time dimension exists)\n",
    "    if 'time' in ds.dims:\n",
    "        time_vals = pd.to_datetime(ds.time.values)\n",
    "        ds[\"hour\"] = ((\"time\",), time_vals.hour)\n",
    "        ds[\"day_of_year\"] = ((\"time\",), time_vals.dayofyear)\n",
    "        # Cyclic encoding\n",
    "        ds[\"hour_sin\"] = np.sin(2 * np.pi * ds[\"hour\"] / 24)\n",
    "        ds[\"hour_cos\"] = np.cos(2 * np.pi * ds[\"hour\"] / 24)\n",
    "        ds[\"doy_sin\"] = np.sin(2 * np.pi * ds[\"day_of_year\"] / 365)\n",
    "        ds[\"doy_cos\"] = np.cos(2 * np.pi * ds[\"day_of_year\"] / 365)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# ========================================\n",
    "# STEP 2: TIME-AWARE DATA SPLITTING\n",
    "# ========================================\n",
    "def temporal_train_test_split(df, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Split time series data temporally (not randomly) to prevent data leakage.\n",
    "    \"\"\"\n",
    "    if 'time' in df.columns:\n",
    "        df_sorted = df.sort_values('time')\n",
    "        split_idx = int(len(df_sorted) * (1 - test_ratio))\n",
    "        train_df = df_sorted.iloc[:split_idx]\n",
    "        test_df = df_sorted.iloc[split_idx:]\n",
    "        print(f\"  → Temporal split: {len(train_df)} train, {len(test_df)} test samples\")\n",
    "        return train_df, test_df\n",
    "    else:\n",
    "        # Fallback to random split if no time column\n",
    "        print(\"  → No time column found, using random split\")\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        return train_test_split(df, test_size=test_ratio, random_state=42)\n",
    "\n",
    "# ========================================\n",
    "# STEP 3: LSTM MODEL ARCHITECTURE\n",
    "# ========================================\n",
    "def create_lstm_model(input_shape, lstm_units=50, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Create LSTM model for wind speed prediction.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units // 2, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def prepare_lstm_sequences(X, y, sequence_length=24):\n",
    "    \"\"\"\n",
    "    Prepare sequences for LSTM training.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(sequence_length, len(X)):\n",
    "        X_seq.append(X[i-sequence_length:i])\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# ========================================\n",
    "# STEP 4: MAIN TRAINING PIPELINE\n",
    "# ========================================\n",
    "def train_models_for_all_levels():\n",
    "    \"\"\"\n",
    "    Train separate models for each pressure level.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"STEP 1: LOADING AND PROCESSING DATA\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Load raw data\n",
    "    print(\"→ Loading GRIB file...\")\n",
    "    ds_raw = xr.open_dataset(GRIB_PATH, engine=\"cfgrib\")\n",
    "    print(f\"  Raw data shape: {dict(ds_raw.dims)}\")\n",
    "    \n",
    "    trained_models = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    # Train a model for each pressure level\n",
    "    for target_level in MODELS_TO_TRAIN:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TRAINING MODEL FOR {target_level} hPa\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # ===== FEATURE ENGINEERING =====\n",
    "        print(\"→ Engineering features...\")\n",
    "        ds_feat = engineer_features_safe(ds_raw, target_level)\n",
    "        \n",
    "        # Add target variable\n",
    "        target_name = f\"ws{target_level}\"\n",
    "        ds_feat[target_name] = ds_feat.wind_speed.sel(isobaricInhPa=target_level)\n",
    "        \n",
    "        # Get feature names (exclude target and intermediate variables)\n",
    "        exclude_vars = {target_name, 'wind_speed', 'u', 'v', 't', 'z'}\n",
    "        ALL_FEATURES = [k for k in ds_feat.data_vars.keys() if k not in exclude_vars]\n",
    "        \n",
    "        print(f\"  → Created {len(ALL_FEATURES)} features\")\n",
    "        \n",
    "        # ===== CREATE DATAFRAME =====\n",
    "        print(\"→ Creating DataFrame...\")\n",
    "        df = (\n",
    "            ds_feat[ALL_FEATURES + [target_name]]\n",
    "            .to_dataframe()\n",
    "            .dropna()\n",
    "            .reset_index()\n",
    "        )\n",
    "        print(f\"  → DataFrame shape: {df.shape}\")\n",
    "        \n",
    "        # ===== LEAKAGE DETECTION =====\n",
    "        print(\"→ Checking for data leakage...\")\n",
    "        X_temp = df[ALL_FEATURES]\n",
    "        y_temp = df[target_name]\n",
    "        \n",
    "        correlations = X_temp.corrwith(y_temp).abs().sort_values(ascending=False)\n",
    "        high_corr = correlations[correlations > 0.95]\n",
    "        \n",
    "        if len(high_corr) > 0:\n",
    "            print(f\"  ⚠️  WARNING: Found {len(high_corr)} features with correlation > 0.95:\")\n",
    "            print(high_corr.head())\n",
    "        else:\n",
    "            print(\"  ✅ No obvious data leakage detected\")\n",
    "        \n",
    "        # ===== TEMPORAL SPLITTING =====\n",
    "        print(\"→ Splitting data temporally...\")\n",
    "        train_df, test_df = temporal_train_test_split(df, test_ratio=0.2)\n",
    "        \n",
    "        X_train = train_df[ALL_FEATURES]\n",
    "        y_train = train_df[target_name]\n",
    "        X_test = test_df[ALL_FEATURES]\n",
    "        y_test = test_df[target_name]\n",
    "        \n",
    "        # ===== FEATURE SCALING =====\n",
    "        print(\"→ Scaling features...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # ===== MODEL TRAINING =====\n",
    "        print(\"→ Training Random Forest...\")\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,  # Limit depth to prevent overfitting\n",
    "            min_samples_split=10,  # Increase minimum samples\n",
    "            min_samples_leaf=5,    # Increase minimum leaf samples\n",
    "            oob_score=True,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # ===== EVALUATION =====\n",
    "        print(\"→ Evaluating model...\")\n",
    "        y_pred_train = rf_model.predict(X_train_scaled)\n",
    "        y_pred_test = rf_model.predict(X_test_scaled)\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        print(f\"\\n  📊 MODEL PERFORMANCE FOR {target_level} hPa:\")\n",
    "        print(f\"     Training   - RMSE: {train_rmse:.3f}, R²: {train_r2:.3f}, MAE: {train_mae:.3f}\")\n",
    "        print(f\"     Testing    - RMSE: {test_rmse:.3f}, R²: {test_r2:.3f}, MAE: {test_mae:.3f}\")\n",
    "        print(f\"     OOB Score  - R²: {rf_model.oob_score_:.3f}\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        overfitting_indicator = train_r2 - test_r2\n",
    "        if overfitting_indicator > 0.1:\n",
    "            print(f\"     ⚠️  Possible overfitting detected (train-test R² gap: {overfitting_indicator:.3f})\")\n",
    "        else:\n",
    "            print(f\"     ✅ Good generalization (train-test R² gap: {overfitting_indicator:.3f})\")\n",
    "        \n",
    "        # ===== FEATURE IMPORTANCE =====\n",
    "        print(\"→ Analyzing feature importance...\")\n",
    "        importances = rf_model.feature_importances_\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': ALL_FEATURES,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"  Top 5 most important features:\")\n",
    "        for idx, row in feature_importance.head().iterrows():\n",
    "            print(f\"     {row['feature']:20s}: {row['importance']:.4f}\")\n",
    "        \n",
    "        # ===== SAVE MODEL =====\n",
    "        model_filename = f\"wind_model_{target_level}hPa.pkl\"\n",
    "        scaler_filename = f\"scaler_{target_level}hPa.pkl\"\n",
    "        \n",
    "        joblib.dump(rf_model, model_filename)\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "        \n",
    "        print(f\"  💾 Saved model: {model_filename}\")\n",
    "        print(f\"  💾 Saved scaler: {scaler_filename}\")\n",
    "        \n",
    "        # Store in dictionary for return\n",
    "        trained_models[target_level] = {\n",
    "            'model': rf_model,\n",
    "            'scaler': scaler,\n",
    "            'features': ALL_FEATURES,\n",
    "            'performance': {\n",
    "                'train_rmse': train_rmse,\n",
    "                'test_rmse': test_rmse,\n",
    "                'train_r2': train_r2,\n",
    "                'test_r2': test_r2,\n",
    "                'oob_r2': rf_model.oob_score_\n",
    "            },\n",
    "            'feature_importance': feature_importance\n",
    "        }\n",
    "        \n",
    "        scalers[target_level] = scaler\n",
    "    \n",
    "    return trained_models, scalers\n",
    "\n",
    "# ========================================\n",
    "# STEP 5: PREDICTION FUNCTION\n",
    "# ========================================\n",
    "def predict_wind_along_path(models, scalers, path_points):\n",
    "    \"\"\"\n",
    "    Predict wind speeds along a UAV flight path.\n",
    "    \n",
    "    path_points: List of dictionaries with keys:\n",
    "        - 'time': timestamp\n",
    "        - 'latitude': latitude in degrees\n",
    "        - 'longitude': longitude in degrees  \n",
    "        - 'altitude_m': altitude in meters\n",
    "    \"\"\"\n",
    "    print(f\"\\n→ Predicting wind for {len(path_points)} path points...\")\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for point in path_points:\n",
    "        # Here you would:\n",
    "        # 1. Load weather data for point's time/location\n",
    "        # 2. Extract features using the same feature engineering\n",
    "        # 3. Apply appropriate model based on altitude\n",
    "        # 4. Interpolate between pressure levels if needed\n",
    "        \n",
    "        # For demonstration, we'll show the structure\n",
    "        altitude_m = point['altitude_m']\n",
    "        \n",
    "        # Convert altitude to approximate pressure level\n",
    "        # (This is simplified - in reality you'd use the geopotential height fields)\n",
    "        if altitude_m < 100:\n",
    "            target_level = 1000\n",
    "        elif altitude_m < 600:\n",
    "            target_level = 950\n",
    "        elif altitude_m < 1200:\n",
    "            target_level = 900\n",
    "        else:\n",
    "            target_level = 850\n",
    "        \n",
    "        # Get the appropriate model\n",
    "        if target_level in models:\n",
    "            model_info = models[target_level]\n",
    "            # prediction = model_info['model'].predict(scaled_features)\n",
    "            # For now, just return the target level used\n",
    "            predictions.append({\n",
    "                'point': point,\n",
    "                'model_used': target_level,\n",
    "                'predicted_wind': None  # Would be actual prediction\n",
    "            })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ========================================\n",
    "# MAIN EXECUTION\n",
    "# ========================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting UAV Wind Prediction Pipeline...\")\n",
    "    \n",
    "    # Train models for all pressure levels\n",
    "    trained_models, scalers = train_models_for_all_levels()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING COMPLETED!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✅ Successfully trained {len(trained_models)} models\")\n",
    "    print(f\"✅ Models available for pressure levels: {list(trained_models.keys())} hPa\")\n",
    "    \n",
    "    # Summary of all models\n",
    "    print(f\"\\n📊 OVERALL PERFORMANCE SUMMARY:\")\n",
    "    print(f\"{'Level (hPa)':<12} {'Test R²':<10} {'Test RMSE':<12} {'Overfitting':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for level, info in trained_models.items():\n",
    "        perf = info['performance']\n",
    "        overfitting = perf['train_r2'] - perf['test_r2']\n",
    "        overfitting_status = \"⚠️ High\" if overfitting > 0.1 else \"✅ Low\"\n",
    "        \n",
    "        print(f\"{level:<12} {perf['test_r2']:<10.3f} {perf['test_rmse']:<12.3f} {overfitting_status}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Next steps:\")\n",
    "    print(f\"   1. Use predict_wind_along_path() function for UAV route planning\")\n",
    "    print(f\"   2. Load models with joblib.load() for deployment\")\n",
    "    print(f\"   3. Integrate with real-time weather data feeds\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PIPELINE READY FOR DEPLOYMENT!\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17875689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = create_cli()\n",
    "    # <-- change this line\n",
    "args, unknown = parser.parse_known_args()\n",
    "if unknown:\n",
    "    logger.debug(\"Ignoring unknown args: %s\", unknown)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "era5_wind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
